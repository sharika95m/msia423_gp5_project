{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::424875905672:role/glue-cdl-full-access\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: d3c0beb3-1cea-43c4-87a8-27aa5078aee6\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session d3c0beb3-1cea-43c4-87a8-27aa5078aee6 to get into ready status...\nSession d3c0beb3-1cea-43c4-87a8-27aa5078aee6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Aux functions",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import *\nfrom pyspark import SQLContext\n\nsqlContext = SQLContext(sc)\n# Auxiliar functions\n# pd to spark (fast)\ndef equivalent_type(f):\n    if f == 'datetime64[ns]': return TimestampType()\n    elif f == 'int64': return LongType()\n    elif f == 'int32' or f == 'uint8': return IntegerType()\n    elif f == 'float64': return DoubleType()\n    elif f == 'float32': return FloatType()\n    else: return StringType()\n\ndef define_structure(string, format_type):\n    try: typo = equivalent_type(format_type)\n    except: typo = StringType()\n    return StructField(string, typo)\n\n# Given pandas dataframe, it will return a spark's dataframe.\ndef pandas_to_spark(pandas_df):\n    columns = list(pandas_df.columns)\n    types = list(pandas_df.dtypes)\n    struct_list = []\n    for column, typo in zip(columns, types): \n      struct_list.append(define_structure(column, typo))\n    p_schema = StructType(struct_list)\n    return sqlContext.createDataFrame(pandas_df, p_schema)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Read in data as dynamic frame\nchurn = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\n        \"paths\": [\"s3://cloud-eng-bucket/\"],\n        \"recurse\": True,\n        \"header\": \"true\"\n    },\n    format=\"csv\"\n)\n\n# Convert to spark df\n# All strings\nchurn_df = churn.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transform",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Fix header\nheader = churn_df.rdd.first()\nchurn_final = spark.createDataFrame(churn_df.rdd.filter(lambda x: x != header), header)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Fix data types\nfrom pyspark.sql.functions import col\n\ncols_to_cast = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges', 'numAdminTickets', 'numTechTickets']\nfor col_name in cols_to_cast:\n    churn_final = churn_final.withColumn(col_name, col(col_name).cast(\"double\"))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# To pd\nimport pandas as pd\nimport numpy as np\n\nchurn_pd = churn_final.toPandas()\n\ndf_2 = churn_pd.copy()\ndf_2['TotalCharges'] = df_2['TotalCharges'].replace(' ',np.nan)\ndf_2 = df_2.dropna(how='any').reset_index(drop=True)\n\nfrom sklearn.preprocessing import OneHotEncoder\ndef get_ohe(df,col_name):\n    ohe = OneHotEncoder(sparse=False,categories=\"auto\",drop=\"first\")\n    ohe.fit(df[col_name])\n    temp_df = pd.DataFrame(data=ohe.transform(df[col_name]), columns=ohe.get_feature_names())\n    df.drop(columns=col_name, axis=1, inplace=True)\n    df = pd.concat([df.reset_index(drop=True), temp_df], axis=1)\n    return df,ohe\n\ndf_3,ohe_obj = get_ohe(df_2,[\"gender\",\"Partner\", \"Dependents\",\"PhoneService\",\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\n                            \"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\n                            \"PaymentMethod\",\"Churn\"])\ndf_4 = df_3.drop(columns = ['customerID'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## All the way back",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\nchurn_clean_spark = pandas_to_spark(df_4)\nchurn_clean_dyf = DynamicFrame.fromDF(churn_clean_spark, glueContext, 'convert')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Write",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Housekeeping\nimport boto3\n\ndatabase_name = \"cloud-eng\"\ntable_name = \"telecom_churn\"\nglue_client = boto3.client('glue')\n\nschema = churn_clean_dyf.schema()\ncolumns = [\n    {\n        \"Name\": field.name,\n        \"Type\": field.dataType.typeName()\n    }\n    for field in schema.fields\n]\n\n# Create table configurations\ncreate_table_options_streamed = {\n    \"DatabaseName\": database_name,\n    \"TableInput\": {\n        \"Name\": table_name,\n        \"Description\": \"Clean data for telecom churn\",\n        \n        \"StorageDescriptor\": {\n            \"Columns\": columns,\n            \"Location\": \"s3://cloud-eng-bucket/\",\n            \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n            \"Compressed\": False,\n            \"SerdeInfo\": {\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n                \"Parameters\": {\n                    \"field.delim\": \",\",\n                    \"skip.header.line.count\" : \"1\"\n                }\n            }\n        },\n        \"PartitionKeys\": []\n    }\n}\n\n# Check if streamed table exists\n# If the streamed table does not exist, create\n\ntry: \n    response = glue_client.get_table(\n    DatabaseName=database_name,\n    Name=table_name\n)\n    print(f\"{table_name} already exists. Directly writing...\")\nexcept:\n    glue_client = boto3.client('glue')\n    response_streamed = glue_client.create_table(**create_table_options_streamed)\n    print(f\"{table_name} does not exist. Creating...\")\n\nglueContext.write_dynamic_frame.from_catalog(\n    frame = churn_clean_dyf,\n    database = database_name,\n    table_name = table_name\n    \n)\n\nprint(f\"Sucessfully wrote to {table_name}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "telecom_churn does not exist. Creating...\nSucessfully wrote to telecom_churn\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}